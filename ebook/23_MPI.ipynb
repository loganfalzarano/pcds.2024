{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI: Message Passing Interface\n",
    "\n",
    "MPI is the programming interface to high-performance computing (HPC), i.e. supercomputers.\n",
    "\n",
    "* Message passing parallelism\n",
    "* Cluster computing (no shared memory)\n",
    "* Process (not thread oriented)\n",
    "* Parallelism model\n",
    "  * SPMD: by definition\n",
    "* MPI environment\n",
    "  * Application programming interface\n",
    "  * Implemented in libraries\n",
    "  * Support for C/C++ and Fortran\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a reasonable tutorial [https://hpc-tutorials.llnl.gov/mpi/abstract/](https://hpc-tutorials.llnl.gov/mpi/abstract/) as part of a High-Performance computing tutorial series. \n",
    "\n",
    "MPI routines that are most useful for new MPI programmers include:\n",
    "\n",
    "* MPI Environment Management \n",
    "* Point-to-Point Communications\n",
    "* Collective Communications\n",
    "\n",
    "Tutorial does not cover advanced topics such as \n",
    "* Derived Data Types\n",
    "* Group and Communicator Management Routines\n",
    "* Virtual Topologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why teach MPI?\n",
    "\n",
    "The communication paradigms, particularly collective communication patterns, are widely used in distributed AI training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPMD: Single program multiple data\n",
    "\n",
    "From wikipedia “Tasks are split up and run simultaneously on multiple processors with different input in order to obtain results faster. SPMD is the most common style of parallel programming.”\n",
    "  * Asynchronous execution of the same program\n",
    "  \n",
    "<img src=\"https://www.sharcnet.ca/help/images/8/8a/SPMD_model.png\" width=512 title=\"SPMD\" />\n",
    "\n",
    "_SPMD_ is not part of Flynn's taxonomy. It is a software programming model. In contrast, SIMD is an architectural classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### A first MPI program\n",
    "\n",
    "* Configure the MPI environment\n",
    "* Discover yourself\n",
    "* Take some differentiated activity\n",
    "\n",
    "all demos in `./mpi` Start with `mpimsg.c`\n",
    "\n",
    "* Idioms\n",
    "  * SPMD: all processes run the same program\n",
    "    * MPI_Rank: tell yourself apart from other and customize the local processes behaviours\n",
    "    * Find neighbors, select data region, etc.\n",
    " \n",
    "The next demo `passitforward.c` builds a simple ring topology and passes the message around the ring.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/36/MPI_Ring_topology.png\" width=256 title=\"MPI Ring\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MPI Toolchain\n",
    "\n",
    "Build and launch scripts that wrap a compiler.\n",
    "    \n",
    "<img src=\"./images/mpiscip.png\" width=400 /> \n",
    "\n",
    "* To compile an MPI program, you call the associated wrapper.\n",
    "* To run an MPI program:\n",
    "  * **debug** `mpirun` to launch MPI job on the local machine/cluster\n",
    "  * **deploy** launch through scheduler on HPC clusters (do not run on the login node)\n",
    "\n",
    "    \n",
    "```\n",
    "mpicc mpimsg.c -o mpimsg\n",
    "mpirun mpimsg\n",
    "mpirun -np 16 --oversubscribe mpimsg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### MPI Runtime\n",
    "\n",
    "MPI programs are just C/Fortran that include message passing directives.\n",
    "One designs an SPMD program that will collaborate to solve a problem that includes:\n",
    "  * Calls to the MPI library\n",
    "  * Interactions with the MPI runtime\n",
    "  \n",
    "Some calls query or manipulate the runtime:  \n",
    "* Initialize the environment\n",
    "  * `MPI_Init ( &argc, &argv )`\n",
    "* Acquire information for process to differentiate process behavior in SMPD\n",
    "  * `MPI_Comm_size ( MPI_COMM_WORLD, &num_procs )`\n",
    "  * `MPI_Comm_rank ( MPI_COMM_WORLD, &ID )`\n",
    "* And cleanup\n",
    "  * `MPI_Finalize()`\n",
    "\n",
    "### MPI Communicators and Groups\n",
    "\n",
    "The MPI runtime has knowledge of the configuration of the cluster. The nodes of the cluster are connected by the global communicator `MPI_COMM_WORLD`. This specifies the number of nodes `MPI_Comm_size`.\n",
    "\n",
    "It is possible to make application/task specific scopes with narrower communicators and groups. For example, you may break the global cluster into nodes with and without GPUs. \n",
    "\n",
    "<img src=\"https://cvw.cac.cornell.edu/mpiadvtopics/communicators-groups/communicators.gif?v=pAsH8Kcxy00ZJiis72tMMunpeqgvBx0cw65cxeq95hw\" width=368 />\n",
    "\n",
    "Most MPI programs and all our examples will use only the global scope.\n",
    "  \n",
    "## MPI Design Ethos\n",
    "* MPI is just messaging.\n",
    "    * And synchronization constructs, which are built on messaging\n",
    "    * And library calls for discovery and configuration\n",
    "* Computation is done in C/C++/Fortran SPMD program\n",
    "* MPI is sometimes called the “assembly language” of supercomputing\n",
    "    * Simple primitives\n",
    "    * Build your own communication protocols, application topologies, parallel execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
