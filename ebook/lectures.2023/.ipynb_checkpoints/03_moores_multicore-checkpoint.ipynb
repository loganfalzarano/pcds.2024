{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicore Processing\n",
    "\n",
    "* **Goal**: Understand multicore architectures.\n",
    "* **Goal**: Understand how the evolution of processing power has lead to multicore computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multicore: Definition and implications\n",
    "\n",
    "A multicore processor is a single integrated circuit (chip) that has multiple processing units (PUs) that execute independent streams of instructions at the same time. Our treatment of multicore will make several assumptions that narrow the class of mutlicore chips to common mutlicore CPUs.\n",
    "  * the processing units are general purpose\n",
    "  * the cores have access to shared memory (this may be main memory or may be a cache)\n",
    "  * parallelism is implemented through both multithreading and multiprocessing\n",
    "  \n",
    "A simple multicore processor looks like:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Dual_Core_Generic.svg/440px-Dual_Core_Generic.svg.png\" title=\"Multicore\" />\n",
    "\n",
    "The physical layout of the transistors of the chip how separate regions implementing separate cores (as seen on https://i.stack.imgur.com/xCqqv.jpg).\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/xCqqv.jpg\" title=\"Multicore annotated die (https://i.stack.imgur.com/xCqqv.jpg)\" />\n",
    "\n",
    "Parallelism is implemented by:\n",
    "  * running separate, independent processes on different cores\n",
    "  * running a single program (process) with multiple threads that perform independent work.\n",
    "  * a combination of both\n",
    "  \n",
    "Threads share memory. Multiple threads in the same program can read and write to the same physical addresses (typically using the same variable names). We will study this in depth.\n",
    "\n",
    "_Given what we know about Amdahl's law, why do we want a multicore processor? Wouldn't it be better to build a single, bigger processor?_ \n",
    "\n",
    "The answer is **YES**. However, this has not happened for a variety of engineering and physics reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moore's Law == More Parallelism\n",
    "\n",
    "Moore’s law -- The number of transistors that can be inexpensively placed on an integrated circuit is increasing exponentially, doubling approximately every two years.\n",
    "\n",
    "* The observation has held for half a century\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/9d/Moore%27s_Law_Transistor_Count_1971-2016.png\" width=\"768\" title=\"Moore's Law\" />\n",
    "\n",
    "Moore's law is commonly misinterpreted as meaning that processing power increases exponentially. This was the case for a long time, but not what Moore's law says.\n",
    "\n",
    "Moore's law continues to hold, but it is not always helpful:\n",
    "\n",
    "* More transistors has become more cores (independent processing units on the same chip)\n",
    "* In 2004, a 'processor' turned into a parallel computer (Itanium 2)\n",
    "* This has progressed to processors with 16 cores, GPUs, etc.\n",
    "\n",
    "The chip vendors tell us we have faster processors, not parallel computers. So we (the programmers) must write parallel code to make software faster on multiple cores with the same clock speed and number of transistors.  \n",
    "  \n",
    "#### Conclusion: \n",
    "\n",
    "Moore's law does not mean that we get more processing power over time. It's not that simple. It's implementation has meant that we get more processing power at the expense of needing to encode more parallelism in programs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dennard Scaling: Why the need for multicore?\n",
    "\n",
    "As transistors get smaller their power density stays constant so that power is in proportion with area.\n",
    "Thus, voltage and current must scale downward.\n",
    "* Performance per watt increases exponentially \n",
    "* Smaller transistors lead to faster clock rates\n",
    "\n",
    "However, Dennard scaling ended in 2006. A about the same time as the advent of multicore.\n",
    "* But Moore’s law still alive\n",
    "* Absent Dennard scaling, only choice was to turn to multicore processors\n",
    "* Could not longer scale voltage and current downward\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Liming-Xiu/publication/330893452/figure/fig1/AS:993832694992898@1613959587856/The-Dennard-scaling-failed-around-the-middle-of-the-2000s-24.png\" width=\"512\" title=\"Dennard Scaling\" />\n",
    "\n",
    "And the situation is getting worse, Apple and NVidia announced that single-chip scaling has reach its limits:\n",
    "* Reticule limited: dies (silicon wafers) cannot become larger without more errors and wastage.\n",
    "* Leakage/Tunneling/Size limitations: 3nm (in production) has long bee held as the limit of photolithography, but TSMC is working on 2nm.\n",
    "\n",
    "There are many paths forward, such as Chiplets, but all of them involved more parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is Moore's Law Dead?\n",
    "\n",
    "Likely yes. We should have an informal discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
