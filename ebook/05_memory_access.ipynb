{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cache Hierarchy\n",
    "\n",
    "* Memory is an abstraction\n",
    "  * looks to processor like a 1-d adress space of data locations\n",
    "  * uniform access from all cores/processors\n",
    "*  Actually a steep, hierarchy of cache in which different levels have different:\n",
    "    * Performance\n",
    "    * Capacity\n",
    "    * Sharing\n",
    "  \n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*42rOo-Rl8seCDV5cZafUNA.png\" width=512 title=\"Cache Hierarchy\" />\n",
    "\n",
    "Image from https://3000rain.medium.com/memory-hierarchy-afb83b61558c\n",
    "\n",
    "* Caches are a place to store a smaller amount of data that is frequently/recently used to make data access faster.\n",
    "  * Processor caches (on chip) is a cache for memory.  Managed by hardware.\n",
    "  * Memory (DRAM) is a cache for pages from disk.  Managed by a storage system (database, file system).\n",
    "  * Management refers to the process of loading and evicting the contents in response to workload.\n",
    "  \n",
    "### The Hierarchy\n",
    "\n",
    "<img src=\"http://www.imexresearch.com/newsletters/images/201009_SSDImages/20100913_SSD_0000.png\" width=512 title=\"IMEX Data on Latency and Cost\" />\n",
    "\n",
    "<img src=\"https://eda360insider.files.wordpress.com/2012/05/wegener-1.gif?w=1400\" width=512 title=\"Cache latency and granularity\" />\n",
    "\n",
    "### Latency\n",
    "\n",
    "Delays (in clock cycles) to different levels in the cache hierarchy for an i7 (Nehalem, 2008).\n",
    " * $1$ cycle to registers (private to each core)\n",
    " * $1$ cycle to L1 (private to each core)\n",
    " * $4$ cycles to L2 (private to each core)\n",
    " * $35$ cycles to L3 (shared by cores)\n",
    " * $145$ cycles to memory (shared by processors)\n",
    " * $10^5$ cycles to NVRAM\n",
    " * $10^7$ cycles to magnetic disk\n",
    "\n",
    "_Data Loading_: New data that has not been used must be loaded from SSD, disk, or memory.\n",
    "\n",
    "_Data Sharing_: When two threads need to share data, they incur the cost of transferring data through the fastest shared cache.\n",
    "  * 2 cores on the same processor take 70 cycles (35 to write to L3 and 35 to read from L3)\n",
    "  * 2 processors take 290 cycles\n",
    "  \n",
    "The following figure is almost right. SMP should really say something like QPI (quickpath interconnect). It is helpful to visualize sharing betweeen cores in L2 and processors in L3.\n",
    "\n",
    "<img src=\"https://www.enterpriseai.news/wp-content/uploads/2014/06/shared-memory-cluster-story-1-processor-cluster.jpg\" width=512 title=\"NUMA schematic from EnterpriseAI\" />\n",
    "\n",
    "This sharing results in _interference_ between processes that share data in OpenMP and threads.  This is the major source for lost parallelism in these programming models.\n",
    "\n",
    "\n",
    "**Cache examples on blackboard.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processor Caching Concepts\n",
    "\n",
    "The memory system should be thought of a a vectorized parallel system.  Whenever you \n",
    "get data, you get many words of data.  To get good memory throughput, you must \n",
    "use all that data.  Most important to understanding cache performance are:\n",
    "* __cache line__: data are moved among levels in the cache one line at a time\n",
    "  * 128 bytes is a typical value for L1 or L2\n",
    "  * each access is a parallel load of an entire line\n",
    "  * good parallel programs will use 64 or 128 bytes\n",
    "* __unified__: refers to whether or not the cache is shared (among cores or processors)\n",
    "\n",
    "Other concepts that don't matter as much.\n",
    "* __inclusive vs exclusive__: has implications for hardware management policies.  We don't care.\n",
    "  * __inclusive__: data in higher level caches are also in lower level caches\n",
    "  * __exclusive__: data in higher level caches are not in lower level caches\n",
    "* __associativity__: the number of hardware locations that a cache line can go into\n",
    "  * important for HW design.  We typically don't care."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Access Patterns\n",
    "\n",
    "* You can’t just access memory\n",
    "  * Different memory access patterns result in large performance differences for the same computation\n",
    "* Worry about:\n",
    "  * Parallelism: am I using all the data in a cache line\n",
    "    * To access a single byte, one must load a whole line\n",
    "    * Sequential access to memory is always parallel!\n",
    "  * Sharing/reuse: is my program referencing data in the cache more than once?  At what levels?\n",
    "\n",
    "Good memory access patterns are __aligned, sequential__ and __coalesced__.\n",
    "  * Aligned – access range starts/ends on cache line boundaries\n",
    "  * Sequential – a continuous range of bytes\n",
    "  * Coalesced – combine multiple small accesses into fewer large accesses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For good memory performance in looping programs\n",
    "  * choose an iteration order that is sequential in memory\n",
    "  * align data\n",
    "      * use addresses that are 0 modulo 128 or 256\n",
    "      * assume that large memory allocations are sequential\n",
    "      * there are specific interfaces to allocate aligned memory (not portable)\n",
    "      \n",
    "      \n",
    "#### Row versus Column Example\n",
    "\n",
    "Example in [row_column.c](./examples/openmp/row_column.c)\n",
    "\n",
    "* Nested loops are a good example\n",
    "  * Row versus column order can make a big difference.\n",
    "  * Think of memory as reading a sequential cache line at a time\n",
    "  \n",
    "<img src=\"./images/rowvcol.png\" width=512 title=\"http://akira.ruc.dk/~keld/teaching/IPDC_f10/Slides/pdf4x/4_Performance.4x.pdf\" />\n",
    "\n",
    "* Reading data a row at a time results in sequential access of all elements.  \n",
    "* Reading successive elements in a column results in strided I/O.\n",
    "    * One element accessed for every column's worth of data.\n",
    "    \n",
    "    \n",
    "We will consider the following two snippets. In the first snippet, $x$ varies fastest. One element of data are access for every $DIM$ elements.\n",
    "    \n",
    "```\n",
    "    for (int y=0; y<DIM; y++) {\n",
    "        for (int x=0; x<DIM; x++) {\n",
    "            array[x*DIM+y] = (double)rand()/RAND_MAX;\n",
    "        }        \n",
    "    }\n",
    "```\n",
    "\n",
    "In this example $y$ is the inside loop and varies fastest. Elements are accessed sequentially in memory\n",
    "\n",
    "```\n",
    "    for (int x=0; x<DIM; x++) {\n",
    "        for (int y=0; y<DIM; y++) {\n",
    "            array[x*DIM+y] = (double)rand()/RAND_MAX;\n",
    "        }        \n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2-d array conventions and programming languages\n",
    " \n",
    "Programming languages that use 2-d array indexing use one of two conventions to serialize array elements to memory.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Row_and_column_major_order.svg/800px-Row_and_column_major_order.svg.png\" width=256 title=\"Row-major versus column-major\" />\n",
    "\n",
    "* Row major systems include python, C++.\n",
    "* Colum major systems include R, Fortran, and image formats.\n",
    "\n",
    "You must be careful when using 2-d indexing.  Class thought exercise. _Write a embedded for loop for python that accesses elements `ar[row][col]` in sequential order._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Sharing\n",
    "\n",
    "Example in [sharing.c](./examples/openmp/sharing.c)\n",
    "\n",
    "Good simple treatment at this [blog](https://haryachyy.wordpress.com/2018/06/19/learning-dpdk-avoid-false-sharing/).  Referring to their diagram.\n",
    "\n",
    "<img src=\"https://haryachyy.wordpress.com/wp-content/uploads/2018/06/false-sharing-illustration.png\" width=384 title=\"False sharing\" />\n",
    "\n",
    "The key thing to understand is that the processors/cores need to exchange synchronization events through the process or main memory system. Another developer [blog](https://learn.microsoft.com/en-us/archive/msdn-magazine/2008/october/net-matters-false-sharing) makes this point.\n",
    "\n",
    "<img src=\"https://learn.microsoft.com/en-us/archive/msdn-magazine/2008/october/images/cc872851.figo2(en-us).gif\" width=384 title=\"False sharing\" />\n",
    "\n",
    "https://learn.microsoft.com/en-us/archive/msdn-magazine/2008/october/images/cc872851.figo2(en-us).gif\n",
    "\n",
    "The four examples:\n",
    "  * each thread writes to same variable (sharing)\n",
    "  * each thread writes to adjacent variables (false sharing)\n",
    "  * each thread writes to different region (no sharing)\n",
    "These examples are just about memory access patterns. We will look at reducers in the next lecture as a way to solve shared write performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LMBench: Understanding Cache Misses\n",
    "\n",
    "LMBench is a suite of performance benchmarking tools written by Carl Staelin and Larry McVoy in 1996. The strided access benchmark still provides the best insight into the structure of cache latencies.  The experiment does the following:\n",
    "\n",
    "> Access a single byte at 128 byte strides (i.e. 0, 128, 256, 384, ...) for an array of a specified size.  Loop over the array multiple times to amortize any initial load costs.\n",
    "\n",
    "For arrays that fit into:\n",
    "  * L1 cache: the L1 cache that contains the entire array and each byte can be accessed in a single clock cycle\n",
    "  * L2 cache but not L1 cache: every byte access transfers a line from the L2 cache to the L1 cache. (Lines are 128 bytes). Each access occurs at L2 latency\n",
    "  * L3 cache but not L2 cache: every byte access transfers a line from L3->L2->L1.\n",
    "  * Larger than L3: performance increases as a function of the working set size. The operating system manages this cache and has access to predictive prefetching and other optimizations.\n",
    "    \n",
    "__Conclusion__: the exact same code at different sizes can have >20x performance differences.\n",
    "  * you have to understand the cache hierarchy and reason about memory access patterns\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LMBench and NUMA:\n",
    "\n",
    "<img src=\"https://sites.utexas.edu/jdm4372/files/2012/03/RangerLatencyChart.jpg\" width=768 title=\"Ranger Memory Performance at TACC\" />\n",
    "\n",
    "For reads that are bigger than L3, there is varying performance depending in multi-processor systems. We will cover this later in a section on __NUMA__=non-uniform memory access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Best Practices\n",
    "\n",
    "* Look at the read/write patterns in you inner loops. Make sure that the access pattern is sequential.\n",
    "* Consider what data is being accessed on a cache miss. Are the other elements in the line useful to the same core?\n",
    "* Avoid false sharing by making sure that each thread/core has a local variable to which it is writing......or separate writes into diferent cache lines.\n",
    "\n",
    "I've assigned the following reading https://siboehm.com/articles/22/Fast-MMM-on-CPU. It is a GREAT treatment. Let's discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
